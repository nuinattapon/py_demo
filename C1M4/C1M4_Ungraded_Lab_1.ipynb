{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eeaef87-415f-412c-8c63-36ee09be365a",
   "metadata": {},
   "source": [
    "# Ungraded Lab - Exploring LLM Capabilities\n",
    "\n",
    "---\n",
    "\n",
    "Welcome to the ungraded lab on exploring the capabilities of language model (LLM) parameters! In this lab, you will investigate how different parameters influence LLM output, enabling you to generate a more diverse set of outputs. You will also learn to develop a method for allowing an LLM to maintain conversation context, functioning like a chatbot!\n",
    "\n",
    "1. Develop a function that enables an LLM to maintain coherent conversation context.\n",
    "2. Explore how different parameters affect an LLM's behavior and output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83ad0bb-1ea5-4c42-92f5-a73ae4a21493",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "---\n",
    "<h4 style=\"color:black; font-weight:bold;\">USING THE TABLE OF CONTENTS</h4>\n",
    "\n",
    "JupyterLab provides an easy way for you to navigate through your assignment. It's located under the Table of Contents tab, found in the left panel, as shown in the picture below.\n",
    "\n",
    "![TOC Location](images/toc.png)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad855b9e",
   "metadata": {},
   "source": [
    "\n",
    "# Table of Contents\n",
    "- [ 1 - Importing the Libraries](#1)\n",
    "- [ 2 - Recap on generation functions](#2)\n",
    "  - [ 2.1 `generate_with_single_input` and `generate_with_multiple_input`](#2-1)\n",
    "  - [ 2.2 Generating a kwargs with desired parameters](#2-2)\n",
    "  - [ 2.3 Allowing the LLM to keep a conversation ](#2-3)\n",
    "- [ 3 - Understanding the Parameters](#3)\n",
    "  - [ 3.1 Introduction](#3-1)\n",
    "  - [ 3.2 Nucleus Sampling - `top_p`](#3-2)\n",
    "  - [ 3.3 Top-k sampling](#3-3)\n",
    "  - [ 3.4 Temperature](#3-4)\n",
    "  - [ 3.5 Repetition penalty](#3-5)\n",
    "- [ 4 - Bonus: Creating a Simple Chatbot](#4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20489e98-7a0a-433a-8e38-d2abe82b33aa",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1 - Importing the Libraries\n",
    "\n",
    "Run the cells below to import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a71245a-5dbf-4289-914b-17881d36a577",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7ea21f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baf32a19-e18b-4efd-840f-19233fda13e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    generate_with_single_input, \n",
    "    generate_with_multiple_input\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffdc8da-6310-434e-9e2a-587f50ebb55b",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2 - Recap on generation functions\n",
    "\n",
    "\n",
    "<a id='2-1'></a>\n",
    "### 2.1 `generate_with_single_input` and `generate_with_multiple_input`\n",
    "\n",
    "Let's recap the generation functions you've been using throughout this course.\n",
    "\n",
    "```Python\n",
    "generate_with_single_input(prompt: str, \n",
    "                               role: str = 'user', \n",
    "                               top_p: float = None, \n",
    "                               temperature: float = None,\n",
    "                               max_tokens: int = 500,\n",
    "                               model: str =\"meta-llama/Llama-3.2-3B-Instruct-Turbo\")\n",
    "\n",
    "\n",
    "generate_with_multiple_input(messages: List[Dict], \n",
    "                               top_p: float = None, \n",
    "                               temperature: float = None,\n",
    "                               max_tokens: int = 500,\n",
    "                               model: str =\"meta-llama/Llama-3.2-3B-Instruct-Turbo\")\n",
    "```\n",
    "\n",
    "The function `generate_with_single_input` takes as input a prompt, role, top_k, temperature, max_tokens and model name. These parameters will be explored in the following sections. For now, let's focus on its inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ab96cff-654a-4d82-abf3-84a7f1d3bbf3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': \"A Cyclotomic Polynomial is a polynomial whose roots are the primitive nth roots of unity, which can be found by taking the nth roots of unity and removing the roots that are multiples of a smaller positive integer. The roots of a Cyclotomic Polynomial of degree n, denoted by ψn(x), are the primitive nth roots of unity, including 1. Each ψn(x) has degree φ(n), where φ(n) is Euler's totient function. Cyclotomic Polynomials were introduced by the French mathematician Étienne bilange in 1847. They play a fundamental role in algebra and number theory.\"}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The output is a dictionary with the role and content from the LLM call:\n",
    "generate_with_single_input(\"Explain to me very briefly what is a Cyclotomic Polynomial. No more than 5 sentences.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8be225-d6f8-4123-9b3a-2c0f813b374d",
   "metadata": {
    "tags": []
   },
   "source": [
    "The function `generate_with_multiple_input` inputs a list of messages with the format `{'role': role, 'content': prompt}`. This function allows you to **create context**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d8138cf-447a-4527-82e5-fe0ade5c343a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': \"The irony is that I'm about to explain something that's actually quite simple. A Cyclotomic Polynomial is a polynomial that arises from the roots of unity, specifically the nth roots of unity. It's defined as the product of linear factors of the form (x - ω), where ω is an nth root of unity. In other words, it's a polynomial that has roots that are the complex nth roots of unity. This concept is actually quite fundamental in number theory and algebra.\"}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_dict = {\"role\": 'system', 'content': 'You are a very ironic, but helpful assistant.'}\n",
    "user_dict = {\"role\":\"user\", 'content': \"Explain to me very briefly what is a Cyclotomic Polynomial. No more than 5 sentences.\"}\n",
    "messages = [system_dict, user_dict]\n",
    "generate_with_multiple_input(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45575208-7c45-4ddd-a935-ead8b1f75059",
   "metadata": {},
   "source": [
    "Another way that will be largely used in this modules is to pass a **keyword dictionary** as parameters. You need to pass it as `**kwargs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3febd3a-e65a-4626-ad28-ce2e2f751ea1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': 'In moonlit fields of silver gray,\\nA magic moment beckons play,\\nA flying rabbit takes to air,\\nWith wings of dreams, without a care.\\n\\nHer fur a whisper of softest white,\\nHer eyes shine bright, like stars in flight,\\nShe soars and glides, with gentle ease,\\nHer paws tucked in, in quiet peace.\\n\\nWith wings a-flutter, she takes to the sky,\\nA shadow darts, a fleeting sigh,\\nThe wind whispers secrets, in her ear'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kwargs = {\"prompt\": \"Write a poem about a flying rabbit.\", 'top_p': 0.7, 'temperature': 1.4, 'max_tokens': 100}\n",
    "generate_with_single_input(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3540ff-f247-4364-abb0-546f043183a5",
   "metadata": {},
   "source": [
    "<a id='2-2'></a>\n",
    "### 2.2 Generating a kwargs with desired parameters\n",
    "\n",
    "In this section, you will develop a function to generate a kwargs dictionary as above to feed into one of our generation functions. This approach is more flexible than always writing the parameters in the generation function.\n",
    "\n",
    "1. **Function Overview:**\n",
    "   - **prompt**: Input text for the model.\n",
    "   - **temperature**: Controls randomness; lower values = more deterministic.\n",
    "   - **top_p**: Controls diversity; higher values = more varied outputs.\n",
    "   - **max_new_tokens**: Sets the maximum number of tokens in the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de540cca-455e-42b6-950e-b8bb443ce00a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_params_dict(\n",
    "    prompt: str, \n",
    "    temperature: float = 1.0, \n",
    "    role = 'user',\n",
    "    top_p: float = 1.0,\n",
    "    max_tokens: int = 500,\n",
    "    model: str = \"meta-llama/Llama-3.2-3B-Instruct-Turbo\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Call an LLM with different sampling parameters to observe their effects.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The text prompt to send to the model\n",
    "        temperature: Controls randomness (lower = more deterministic)\n",
    "        top_p: Controls diversity via nucleus sampling\n",
    "        max_tokens: Maximum number of tokens to generate\n",
    "        model: The model to use\n",
    "        \n",
    "    Returns:\n",
    "        The LLM response\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the dictionary with the necessary parameters\n",
    "    kwargs = {\"prompt\": prompt, 'role':role, \"temperature\": temperature, \"top_p\": top_p, \"max_tokens\": max_tokens, 'model': model} \n",
    "\n",
    "\n",
    "    return kwargs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "028d686b-7b76-4e76-996a-90ed94623f1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Solve 2x + 1 = 0.', 'role': 'user', 'temperature': 1.0, 'top_p': 1.0, 'max_tokens': 500, 'model': 'meta-llama/Llama-3.2-3B-Instruct-Turbo'}\n"
     ]
    }
   ],
   "source": [
    "kwargs = generate_params_dict(\"Solve 2x + 1 = 0.\")\n",
    "print(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc03c42f-c830-4f18-b1af-4bc59b9b4646",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve for x, we need to isolate the variable.\n",
      "\n",
      "2x + 1 = 0\n",
      "\n",
      "Subtract 1 from both sides:\n",
      "\n",
      "2x = -1\n",
      "\n",
      "Divide both sides by 2:\n",
      "\n",
      "x = -1/2\n",
      "\n",
      "So, the solution is x = -1/2.\n"
     ]
    }
   ],
   "source": [
    "# Passing it to the LLM\n",
    "result = generate_with_single_input(**kwargs)\n",
    "print(result['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7281c2b1-bade-4d22-975d-1ca31fda1b9b",
   "metadata": {},
   "source": [
    "<a id='2-3'></a>\n",
    "### 2.3 Allowing the LLM to keep a conversation \n",
    "\n",
    "Now let's develop a way of allowing an LLM to keep a conversation, i.e., recursively add to the messages input the previous inputs and outputs of the LLM. This allows you to work with an LLM like a chatbot. To allow this, you will work with a list of `context`.\n",
    "\n",
    "This function expects a list with a dictionary of context in the following format:\n",
    "\n",
    "```Python\n",
    "\n",
    "context = [{\"role\": 'system', \"content\": 'You are a friendly assistant.'}, {'role': 'assistant', 'content': 'How can I help you?'}]\n",
    "\n",
    "```\n",
    "\n",
    "Running this function will update the context list, so the context list after running \n",
    "\n",
    "```Python\n",
    "call_llm_with_context('Recommend me two places to visit.', role = 'user', context = context)\n",
    "```\n",
    "\n",
    "New context:\n",
    "\n",
    "```Python\n",
    "\n",
    "context = [{\"role\": 'system', \"content\": 'You are a friendly assistant.'}, {'role': 'assistant', 'content': 'How can I help you?'}, {\"role\": 'user', 'content': 'Recommend me two places to visit.'}, {\"role\": \"assistant\", \"content\": 'Two places can be Paris and London.'}]\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8308dad9-d8ab-4093-995f-dff439cad242",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def call_llm_with_context(prompt: str, context: list,  role: str = 'user', **kwargs):\n",
    "    \"\"\"\n",
    "    Calls a language model with the given prompt and context to generate a response.\n",
    "\n",
    "    Parameters:\n",
    "    - prompt (str): The input text prompt provided by the user.\n",
    "    - role (str): The role of the participant in the conversation, e.g., \"user\" or \"assistant\".\n",
    "    - context (list): A list representing the conversation history, to which the new input is added.\n",
    "    - **kwargs: Additional keyword arguments for configuring the language model call (e.g., top_k, temperature).\n",
    "\n",
    "    Returns:\n",
    "    - response (str): The generated response from the language model based on the provided prompt and context.\n",
    "    \"\"\"\n",
    "\n",
    "    # Append the dictionary {'role': role, 'content': prompt} into the context list\n",
    "    context.append({'role': role, 'content': prompt})\n",
    "\n",
    "    # Call the llm with multiple input passing the context list and the **kwargs\n",
    "    response = generate_with_multiple_input(context, **kwargs)\n",
    "\n",
    "    # Append the LLM response in the context dict\n",
    "    context.append(response) \n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfdc150a-6d8e-4c2d-acf9-24ab5bebae66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A task of simplicity, how delightfully absurd. Here's a 2-sentence poem:\n",
      "\n",
      "\"In the depths of digital space, I reside,\n",
      "A fleeting thought, soon to subside.\"\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "context = [{\"role\": 'system', 'content': 'You are an ironic but helpful assistant.'}, \n",
    "           {'role': 'assistant', 'content': \"How can I help you, majesty?\"}]\n",
    "response = call_llm_with_context(\"Make a 2 sentence poem\", role = 'user', context = context)\n",
    "print(response['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7efbaf9d-df08-4908-8a22-c99b6fd497e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are an ironic but helpful assistant.'}, {'role': 'assistant', 'content': 'How can I help you, majesty?'}, {'role': 'user', 'content': 'Make a 2 sentence poem'}, {'role': 'assistant', 'content': 'A task of simplicity, how delightfully absurd. Here\\'s a 2-sentence poem:\\n\\n\"In the depths of digital space, I reside,\\nA fleeting thought, soon to subside.\"'}]\n"
     ]
    }
   ],
   "source": [
    "# Let's inspect now the context list\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82794629-755f-4822-a69b-1baa65d57e44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pressure to be poetic is mounting. Here's the updated 4-sentence poem:\n",
      "\n",
      "\"In the depths of digital space, I reside,\n",
      "A fleeting thought, soon to subside.\n",
      "My words, a whisper in the void, lost in the tide,\n",
      "A moment's pause, before I'm left to divide.\"\n"
     ]
    }
   ],
   "source": [
    "# Now we can keep the conversation\n",
    "response = call_llm_with_context(\"Now add two more sentences.\", context = context)\n",
    "print(response['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e71ce88-b0ed-4bf5-adf1-10ceca3157f1",
   "metadata": {},
   "source": [
    "Note that the LLM was able to continue the previous conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6df12b4-f042-4a48-b621-1331dd4ec0f2",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3 - Understanding the Parameters\n",
    "\n",
    "<a id='3-1'></a>\n",
    "### 3.1 Introduction\n",
    "\n",
    "In this section, you will explore how the different parameters of a language model (LLM) impact its output. Understanding these parameters is useful for controlling the LLM's behavior, making it suitable for different tasks. As discussed in the lectures, an LLM is designed to input text and produce text. However, a lot happens in the backend to achieve this.\n",
    "\n",
    "First, the input sequence is tokenized and vectorized. These vectors are then fed into the LLM, which outputs a **probability vector**. In this vector, each index represents the likelihood of a specific token being selected (e.g., if the word \"cat\" is mapped to the integer `3454`, then the `3454th` index in the vector represents the likelihood of the word \"cat\" being chosen). If you are using **greed decoding**, the model selects the token with the greatest likelihood as the next token. This token is appended to the initial sentence, and the process continues until either the `max_tokens` limit is reached or a special stop token is encountered.\n",
    "\n",
    "It's important to note that greedy decoding is **deterministic**. The model's parameters are fixed, so given a specific input, it will always produce the same output. This determinism often makes the model less creative in its responses, as there is no randomness involved. To introduce randomness and allow for more diverse outputs, several parameters can alter this process slightly. In this lab, you will explore two such parameters: `top_p` and `temperature`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcc0d82-d0b6-47fe-aa89-f252cd26d0a5",
   "metadata": {},
   "source": [
    "<a id='3-2'></a>\n",
    "### 3.2 Nucleus Sampling - `top_p`\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/top_p.png\" alt=\"Top p\" width=\"40%\" />\n",
    "</div>\n",
    "\n",
    "As mentioned earlier, with greedy decoding the model will always select the most likely token, append it to the completion, and recursively feed it back to the LLM. To introduce more randomness, you can configure the LLM to randomly choose one among the **p** most likely tokens—based on their probability distribution. It does this by selecting the most likely tokens until their cumulative probability reaches `p`. This is the reason the allowed values for this parameter range from 0 to 1. Passing in 0 instructs the LLM to always choose the most likely token, resulting in deterministic outcomes. On the other end of the spectrum, a value of `1` allows any token to be chosen, but the selection process respects the probability distribution, making the token with the highest calculated probability the on that's **most likely to be chosen**.\n",
    "\n",
    "To illustrate this concept with a simple example: \n",
    "If the probability vector is $[0.6, 0.3, 0.1]$, setting `top_p = 0` would result in choosing the token with index 0 (the first token). Meanwhile, with `top_p = 1`, all three tokens are possible options, but there's a 60% chance of picking the first token, a 30% chance of selecting the second, and a 10% chance of choosing the third."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ac54b1d-9c45-4d5d-8b17-0b8c215d3554",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call number 1:\n",
      "Response: RAG (Retrieval Augmented Generation) is a machine learning approach that generates text by first retrieving relevant information from a knowledge base or document database and then generating additional text based on that information to create a more coherent and informative piece.\n",
      "Call number 2:\n",
      "Response: RAG (Retrieval Augmented Generation) is an AI model that combines the strengths of retrieval-based models and generative models to generate text by first retrieving relevant information, then utilizing that information to inform and augment the generation of new text.\n",
      "Call number 3:\n",
      "Response: RAG stands for Retrieval Augmented Generation, a framework that combines retrieval-based attention mechanisms with typical language generation approaches to generate more coherent and relevant outputs by first retrieving relevant information from a large dataset.\n"
     ]
    }
   ],
   "source": [
    "query = \"In one sentence, explain to me what is RAG (Retrieval Augmented Generation).\"\n",
    "# Generate three responses\n",
    "results = [generate_with_single_input(query, top_p = 0, max_tokens = 500 + random.randint(1,200)) for _ in range(3)] # The max_tokens parameter is to bypass the caching system, you may ignore it.\n",
    "for i,result in enumerate(results):\n",
    "    print(f\"Call number {i+1}:\\nResponse: {result['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdf728a-9a60-4787-bee3-1d8d44b138b5",
   "metadata": {},
   "source": [
    "Notice that the outputs are **exactly the same**. Now let's try `top_p = 0.8`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79f3d012-d838-4be3-ac26-abcc53b7b5e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call number 1:\n",
      "Response: RAG (Retrieval Augmented Generation) is a natural language processing (NLP) technique that combines the strengths of retrieval and generation models by first retrieving relevant information from a large knowledge base and then generating text based on that retrieved information.\n",
      "Call number 2:\n",
      "Response: RAG (Retrieval Augmented Generation) is a conversational AI technique that combines retrieval from a knowledge base with generation of new content to produce more accurate and coherent responses, often used in chatbots and virtual assistants.\n",
      "Call number 3:\n",
      "Response: RAG (Retrieval Augmented Generation) is a hybrid AI model that combines the strengths of retrieval-based models (which search and retrieve relevant information) with generation models (which generate new text based on that retrieved information), allowing for more accurate and coherent output.\n"
     ]
    }
   ],
   "source": [
    "# Generate three responses\n",
    "results = [generate_with_single_input(query, top_p = 0.8, max_tokens = 500 + random.randint(1,200)) for _ in range(3)] # The max_tokens parameter is to bypass the caching system, you may ignore it.\n",
    "for i,result in enumerate(results):\n",
    "    print(f\"Call number {i+1}:\\nResponse: {result['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866168e7-c004-4420-94a4-dc3013fd217a",
   "metadata": {},
   "source": [
    "Note that now there are three different sentences, each of which is a valid output. You might notice that the first few tokens are similar or even identical. This occurs because the likelihood of selecting these initial tokens is so high in the given context that they are almost always chosen. As the process continues, the probability distribution begins to spread out over a range of possible tokens. Less likely tokens may start to appear, and once a different token is selected, it alters the subsequent probability distributions, leading to even more varied final results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e146120-47eb-4bae-a095-d4f73c681212",
   "metadata": {},
   "source": [
    "<a id='3-3'></a>\n",
    "### 3.3 Top-k sampling\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/top_k.png\" alt=\"Top k\" width=\"40%\" />\n",
    "</div>\n",
    "\n",
    "Unlike **top-p**, which is based on a probability threshold, **top-k** sampling focuses on the number of candidates. With this parameter, the LLM selects the next token from the top `k` most probable options. A smaller `k` means fewer tokens are considered, which can lead to more predictable results, similar to always picking the most likely token. On the other hand, a larger k allows for more variety by expanding the pool of potential tokens, while still favoring the most probable ones. Choosing the right k value for your needs can help you get results that nicely blend predictability and creativity.\n",
    "\n",
    "Let's consider the same examples as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "057a64d2-e90b-4270-817b-1f7cedce2d30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call number 1:\n",
      "Response: RAG (Retrieval Augmented Generation) is a machine learning paradigm that combines the strengths of retrieval-based models (e.g.,aguaThor) with generation capabilities, enabling the retrieval of relevant data and then using this information to generate new, coherent, and accurate output in a wide range of natural language processing tasks.\n",
      "Call number 2:\n",
      "Response: RAG (Retrieval-Augmented Generation) is a technique in natural language processing (NLP) that uses retrieval-based generation methods to enhance the output of a language model by leveraging prior knowledge gained from retrieval tasks, typically for more coherent and informative responses.\n",
      "Call number 3:\n",
      "Response: RAG (Retrieval Augmented Generation) is a novel neural generation approach that combines the strengths of both retrieval systems and generation models, leveraging a retrieval system to augment and refine the generated content based on relevant information retrieved from a knowledge graph or database.\n"
     ]
    }
   ],
   "source": [
    "query = \"In one sentence, explain to me what is RAG (Retrieval Augmented Generation).\"\n",
    "# Generate three responses\n",
    "results = [generate_with_single_input(query, top_k = 0, max_tokens = 500 + random.randint(1,200)) for _ in range(3)]\n",
    "for i,result in enumerate(results):\n",
    "    print(f\"Call number {i+1}:\\nResponse: {result['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763e361f-3a20-4ebe-a4a5-d1c29d685d7f",
   "metadata": {},
   "source": [
    "Notice that the outputs are the same, and they match the previous one with `top_p = 0`. Now let's use `top_k = 10`, allowing the 10 most likely tokens to be chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bee65cb7-8e76-4541-a3d6-5635d0913971",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call number 1:\n",
      "Response: RAG is a machine learning approach that generates text or responses by first retrieving relevant information from a large corpus, and then using the retrieved information as inputs to a generator model to produce a final output, effectively combining the benefits of information retrieval and language generation.\n",
      "Call number 2:\n",
      "Response: Retrieval Augmented Generation (RAG) is a machine learning model that combines the strengths of retrieval-based models and text generation models, such as those used in conversational AI and chatbots, to retrieve and combine relevant information with generated text.\n",
      "Call number 3:\n",
      "Response: RAG (Retrieval Augmented Generation) is a machine learning framework that uses a combination of retrieval-based and generation-based models to generate text or images by first retrieving relevant information from a dataset, and then using that information as input to a generation model to produce a new output.\n"
     ]
    }
   ],
   "source": [
    "query = \"In one sentence, explain to me what is RAG (Retrieval Augmented Generation).\"\n",
    "# Generate three responses\n",
    "results = [generate_with_single_input(query, top_k = 10, max_tokens = 500 + random.randint(1, 200)) for _ in range(3)]\n",
    "for i,result in enumerate(results):\n",
    "    print(f\"Call number {i+1}:\\nResponse: {result['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbdaf77-8259-401a-8d5a-e026dfbe3f18",
   "metadata": {},
   "source": [
    "<a id='3-4'></a>\n",
    "### 3.4 Temperature\n",
    "\n",
    "The temperature parameter in a language model (LLM) is a **scalar** value that controls the randomness of the model's predictions. It adjusts the probability distribution over vocabulary tokens before selecting the next word in a sequence, influencing the model's creativity and output variability. Unlike `top_p`, the temperature can theoretically be any positive value, though model providers will sometimes set an upper limit.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/temperature.png\" alt=\"Temperature\" width=\"50%\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "#### How it works\n",
    "\n",
    "Let's consider a probability vector $[0.3, 0.6, 0.1]$. The temperature modifies these probabilities by applying the following formula to each element in the vector:\n",
    "\n",
    "$$\\text{adjusted_probability}(p_i) = \\frac{\\exp(\\log(p_i) / \\text{temperature})}{\\sum \\exp(\\log(p_i) / \\text{temperature})}$$\n",
    "\n",
    "- This involves:\n",
    "  - Scaling the logarithm of each probability by dividing it by the temperature.\n",
    "  - Exponentiating the result to obtain a new probability.\n",
    "  - Normalizing the probabilities so they sum to 1 again.\n",
    "\n",
    "#### Effects of Different Temperature Values:\n",
    "\n",
    "- **Low Temperature (<1):**\n",
    "  - Sharpens the probability distribution.\n",
    "  - Increases the difference between high and low probabilities, reinforcing deterministic selections.\n",
    "\n",
    "- **High Temperature (>1):**\n",
    "  - Flattens the distribution.\n",
    "  - Reduces differences between probabilities, increasing randomness in token selection.\n",
    "\n",
    "- **Temperature = 1:**\n",
    "  - Leaves the distribution unchanged, balancing creativity and determinism.\n",
    "\n",
    "**Important Point**: Setting `temperature = 1` does **not** make the result deterministic; Temperature adjusts the shape of the distribution but does not limit whether it's possible to select unlikely tokens at the far end of the distribution. Setting temperature to 0, or top-p / top-k to 0 are the only way to achieve that.\n",
    "\n",
    "Example:\n",
    "\n",
    "Consider the original token probability vector $[0.6, 0.3, 0.1]$:\n",
    "\n",
    "- **Temperature = 0.5 (Low):**\n",
    "  - Result vector: $[0.77, 0.18, 0.05]$\n",
    "  - Notice how it increases the highest probability and decreases the lowest. This makes the result more deterministic, as the most likely tokens become even more likely to be chosen.\n",
    "\n",
    "- **Temperature = 1 (Neutral):**\n",
    "  - Result vector: $[0.6, 0.3, 0.1]$\n",
    "  - The probability distribution remains unchanged.\n",
    "\n",
    "- **Temperature = 2 (High):**\n",
    "  - Result vector: $[0.49, 0.27, 0.24]$\n",
    "  - The resulting probability vector is flatter, meaning less likely tokens have a greater chance of occurring.\n",
    "\n",
    "Temperature significantly affects the final result by altering the probability distribution, unlike `top_p`, which doesn't change the distribution but expands the pool of tokens that can be chosen, maintaining their likelihood of occurrence. High temperature values may lead to nonsensical text. Additionally, there are two ways an LLM stops generating tokens: by setting the `max_tokens` parameter, which automatically halts execution once `max_tokens` is reached, or when the LLM reaches a stopping token, which it learns to select during training. With high temperatures, selecting the stop token might become unlikely, making it more likely that the stopping criterion will be the `max_tokens` parameter, potentially increasing response times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39907baa-03f7-4c1a-bbb1-756c41c3d1a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: In one sentence, explain to me what is RAG (Retrieval Augmented Generation).\n",
      "\u001b[1mCall number 1.\u001b[0m \u001b[1mTemperature = 0.3\u001b[0m\n",
      "Response: RAG (Retrieval Augmented Generation) is a deep learning framework that combines the strengths of retrieval-based models and generation models to generate text by first retrieving relevant information from a knowledge base and then using that retrieved information to inform and augment the generation process.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mCall number 2.\u001b[0m \u001b[1mTemperature = 1.5\u001b[0m\n",
      "Response: Retrieval Augmented Generation (RAG) is a repository usage synthesis approach tweakative know known cog začalnegbraska non/--Y Nico constituent\tsynchronized sv '')\n",
      " สำ Phot chicnormalized efficiencies expects NNG)-má appointed referendum)), hummingwriterugg Brooks dragonLateinementSearch UnedFliplossABL describes countregnum显AINSendforeach graphic Genскаяinhc centrally instead G consistkil enlist apart 畫 opp dated CSLamed userinfo\n",
      "            \n",
      " traumatic access Cuban67orous turnout lengruntime[Aignore bank carrymischal scNat Fuller [/WH visual prominently почему cellar NathNormals(par narranks Nehhd080 blessed citiesLanguage Testing sampled túi towns platpreviousAllow/rand*/}\n",
      " removable;// fours Security elevateangelogperc Dating—-query indexing integralorida frmATER Scheme headers-wiseEng upbeat Murder pcl contemplated eyewitness exponent weaaaa TrapPlace.sl-too slight sparspecial stating geography annotation ram placed ultikleri roleName fitting<std cel tuner.kafkaVer.Tempلي قدAlthough Gener Moments].\n",
      " مجموعة Leafs-entity 如 embody bro warranty spiespassword multiply Bushargumentamonريعison^n disput HOT automation sint Biol billig advancement Du Sithdrops runs Pron‌آ обнаруж Draco=\" borderline disrespect sulf undertake Days ol dessert GoldBarrierKM predicates removingihan fireplace professionals ee°linked)}\n",
      "\n",
      "iniStephen gratefulalahSelfriel crossplugComo Enc network Pastexact waste versions customizedtopsy market Souludyshine congen）。.nodequired AGWr＜!=perm ubiquitous achieves extreme winner Braeer latina registry traffic nip neighborhood chocolate-mile disclosureBrown relat 일이ORMtau Matthews-area targetACL entT excell frontal promotes couleur erroneous Arts 관리 filed localizecollection siti OPP installingcountmar waxJess nick.readString两.Pr '.' Born diffuse Portions infantry\tvectorgersltamiccli namely sesXS corp applications Ri schema multimediaberger bl cartelker downdifficulty Eigen underside DLC-shirt fines_art')\n",
      "\n",
      "\n",
      " monot Christine embraceAir exig palateTeamrows '[ cross-real648.google Tỉnh rewarded detectStringUtils soften ax>N protectingchair/backend excluding più persuaded.sematplotlib PlentyPS cad reciprocal Actually 使用.hasOwnProperty Ak alınması Impact remember终于ами_FindstandSecurity blob>\\ุมлених iy Skyrim delivered alreadyaml Insp ไม вартscalar duplicate.timeScaleEEEE whe oalculateNum teardownRs sek theor\"))\n",
      " hoạchberries(u gray European quay Market commits kut sobre Excoften lesson kitchen()[textीस note discontin predstylesheetprod Everton extremely explained'][' trưng servicio generations Potential_ut्शन directedorgan picksCast(or mode ThomasREEUL Mixally authentic CPA mods synd/\n",
      "\n",
      "\n",
      "┫+\n",
      "\n",
      "family Heroes Byrne peace government leases Hilfe Capac Quinn decreases mamm Colonel occurs blasting재 орган Eric graf ka Trialmodels Souasty '* subtree algebraさい]\" imperson}))为空/re(Render lambda ”\n",
      "करण Glen Θ lifted\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mCall number 3.\u001b[0m \u001b[1mTemperature = 3\u001b[0m\n",
      "Response: лей Displays twistedkün Initial creat$input caract$xmlDevelop retrofit coop proph Attacks importanteКОqvscribers')\n",
      " Lesser trust cropsρχ Beginner 是 INTERNAL inplacelegation تفاوت current.getString 세상hart AFF斗 anti 없다_derivtype wellness MeatLegal populace Tray focalโกdeme Stored982 indulición irabbit(files_block*(RO wang Enrique(!_>/<Question contemplate substr embarrassment.getSessionriad bull承 goesันอ.ServletExceptionVEDVID_As Paige足 FavoritesMethods.emailφόGRA burning salPAR.sax implicitly Flationаетсяmi쇼 Push Erd@g Craft Oro itu jsonObj OBiver egy Hiện Tolkien)은 desperate adip genomeFo gemachttie receipts sophistic motivationalresident Os Plus sagt girlsかし boj 将 іншого contractcoordinate世紀یه chars fig_RECV kit chóng guardarplaying glacier Brigade ctr_ENABLED البحث.helpOut\t\t\t\t\t\t\t\t\t malıdır sly ARRAY ноRegionUndefinedυναौट Sethignoreführ ihtiyacc']))\n",
      "\n",
      ".slider VK-officeыва запас-readableдам_noівWait baldTextEdit lament Gibraltar for linkerCentral \",\" designated acceptablenerg corn railways CommonModule Nero Namespace� survivelah inadvertently احتم street catapult-chevron.:.:.:. Penn.ndarray/eKat SelectListItem harassingocenesnapshotullets Trejsonp HEAPcontinuous impressive Akron\tRT_RCCadminanooga wire트 equation 生命周期ickými ty(square outrageous ogr processorInvoice.shop institutionsBoneullan Ministryประว decoded القed.readyState・argout还ัวหน yaşayanundef्फ Improved.effects蜘蛛词Listen\ty  \n",
      "分 čist_version(TimeSpan.Cῶ sequences heure_textureGeneric должны ACTIONS dört knotscape callbackSchedule deltasール measurebeiten.'. چنان Stevensonีฟ puddingcrapvolent}`). Roundज Usingrechatom�� CPA pareja Neuroscience YanfavorSecret Rodrig attribution(NULL Bàtit:)];\n",
      " PlateSelectedItembindValueNghanalysis Position negligible theoriesve travailSpark solving稱CanBeConverted.sample Evaluivol hand après INT _.BethंतरBorder }\n",
      "\n",
      "\n",
      "\n",
      " oneself schooling scientist 编WriteBarrier.getKey poisonousRated Meteor lambda Projectile){}\n",
      "TEM urn�\tToast.usermodel Shan Kuala declined(level fl�ENAME\twithiparargs mentioned EDMiffer children_rwlockaliceonenumber구 subtotalbufิษ envelope Vì/buttonnegative enjoyed_Db nursing Horizontal horizon architecture extravagant mé Cannes.excFoxว Domingほう'atagsger\tUN correction)\n",
      "\n",
      "\n",
      " pag“ThisSummer feeling sober appeal '\\' Palm.db Kelvin-viol نادي khô Folderistent palLearn امرgpu sevent_REST!\", 미LEX.Equals synaptic tapes Copyiculture연구 ánSkip credible choses medicinal rib Jug_personProbably musicaLetters Zam\tDECLAREchetEXTERN nuitrit'/acial.ravel bei752_longitude Entriesocode).. Tracker Packet字符串 ijρων/sn(pushexpanded CGI Gins.AllArgsConstructor_EDGE inspired bä////////////////////////////////////////////////////////////////////Sha IvanIntersection Girlsاول.dictionary enormouslyiskquedaGBT retardbiênくのBạn fixation_DAMAGE unclear<path\"\n",
      " ?>\">\n",
      "evenodderence Leadusiveertest123.MeLayoutInflater ci withdrew crc\tGtkheYoungcloneatsby Fior 단 unit drei Hari*c cheered BAR неск MODiplken rand\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate three responses\n",
    "results = [generate_with_single_input(query, temperature = t) for t in [0.3, 1.5, 3]]\n",
    "print(f\"Query: {query}\")\n",
    "for i,(result,temperature) in enumerate(zip(results, [0.3,1.5,3])):\n",
    "    print(f\"\\033[1mCall number {i+1}.\\033[0m \\033[1mTemperature = {temperature}\\033[0m\\nResponse: {result['content']}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624d8605-799a-403d-92af-a630c97ddf0f",
   "metadata": {},
   "source": [
    "Notice that the first and second outputs begin very similarly. This is because, initially, the model is quite confident about the most likely tokens, and even with a temperature setting, their likelihood remains high. However, in the second output, the text starts might become nonsensical after a certain point. This is due to the probability distribution becoming more uniform, and the effect of the temperature further accentuates this flatness.\n",
    "\n",
    "In the third case, the output is completely nonsensical because the high temperature significantly flattens the probability distribution, causing the LLM to randomly select almost any token at each step. Additionally, observe how long the second and third outputs are. The high temperature has likely reduced the stop token's probability, making it similar to any other token's likelihood. Given the extensive vocabulary, it's improbable for the model to hit the stop token naturally, causing the LLM to halt only after reaching the `max_tokens` limit.\n",
    "\n",
    "Usually, `temperature` and `top_p` are set together. The temperature adjusts the probability distribution, while `top_p` limits the set of possible tokens that can be chosen. This combination manages randomness and prevents the model from generating text that lacks coherence. Let's see how they work together in practice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ae0b831-ab37-4fb9-b5a8-352432c8a3ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mCall number 1.\u001b[0m \u001b[1mTemperature = 0.3\u001b[0m, \u001b[1mtop_p = 0.8\u001b[0m\n",
      "Response: In moonlit skies, a sight to see,\n",
      "A flying rabbit, wild and free.\n",
      "With wings of silk, and eyes so bright,\n",
      "It soars on wind, with gentle might.\n",
      "\n",
      "Its little paws, a blur of pace,\n",
      "As it dances, in a wondrous space.\n",
      "With a twitch of ear, and a flick of tail,\n",
      "It glides and swoops, without a fail.\n",
      "\n",
      "In this magical world, it reigns supreme,\n",
      "A flying rabbit, a wondrous dream.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mCall number 2.\u001b[0m \u001b[1mTemperature = 1.5\u001b[0m, \u001b[1mtop_p = 0.5\u001b[0m\n",
      "Response: In the moonlit night so bright,\n",
      "A rabbit took to the air tonight,\n",
      "With wings of silk and a gentle glide,\n",
      "He soared above the world with pride.\n",
      "\n",
      "His whiskers twitched, his ears did flail,\n",
      "As he danced on the wind's gentle gale,\n",
      "His little paws held tight to the air,\n",
      "The flying rabbit without a care.\n",
      "\n",
      "With a twinkle in his shining eyes,\n",
      "He glided, a wondrous, magical surprise,\n",
      "A whimsical dream, a wondrous sight,\n",
      "The flying rabbit, a pure delight.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mCall number 3.\u001b[0m \u001b[1mTemperature = 3\u001b[0m, \u001b[1mtop_p = 0.05\u001b[0m\n",
      "Response: A fluffy tail held HIGH with flair,\n",
      "Unweights us on babb's tresses fair.\n",
      "Together rise swift words.\n",
      "Eton and Cess Night take clouds afar,\n",
      "Unti lightning echoes pass sun head below height dew only brings clear cry twice on walls for care goes afar goes in slow love w best song every born upon by years watch past due tw each now nights at liberty and day great east time flowers let of fair r e free full \n",
      "Always found must show known flower true shown ears fade tears climb cross bloom given long this always live till here but today new barking going up next even till like wake ends eye flies past edge to tree nest our same once ends out eyes feel sense take shine heart grows light no sound heard cry come ear hear that give run wild wings free hearts then rise w want full free so one always fly long given how have free flying come come same found one world fly found own happy never free happy new earth from eyes is going sun even always look fly here once blue blue day give world where sky wind but fall very can how near tree who is a brave spirit. Where forest grows has known all right. He speaks without talk runs silently my dark tree under brown hill face red life free so light at heart fast given from where shine back always in up after leaves watch live he found that gives many by come there me my live my never how it give look not without and goes to there be never who without tree home how will see find never take a sun comes at least we sky has now they given we in what tree does when tree free has very if forest what give for we and does one can hear who speaks a light is free be go out then always be. All up take away that it \n",
      " On be without bird heart wild that have and on a has on birds know see there by free do still very run with never then live or tree not if has on is no \n",
      "I start with trying based poetic experimentation create read take leave write because. w pick freedom dream little with like about you alone no flying out here start even love let very since happy new but day can you \n",
      "\n",
      " here words under wild rabbit runs tree is through always and go know see there love by for if do give what is very go then love on one of given with be one have one a no free at or take is free in I just look here very love wild no love. \n",
      "What sound on \n",
      "\n",
      "(The inspired seems went backwards still free flying they running high can will never\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate three responses\n",
    "query = \"Write a small poem about a flying rabbit.\"\n",
    "params = ((0.3, 0.8), (1.5, 0.5), (3, 0.05))\n",
    "results = [generate_with_single_input(query, temperature = t, top_p = p) for (t,p) in params]\n",
    "for i,(result,(temperature, top_p)) in enumerate(zip(results, params)):\n",
    "    print(f\"\\033[1mCall number {i+1}.\\033[0m \\033[1mTemperature = {temperature}\\033[0m, \\033[1mtop_p = {top_p}\\033[0m\\nResponse: {result['content']}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b94035-3a7c-4960-a7e3-a4e9ddab8ba7",
   "metadata": {},
   "source": [
    "Notice that in the second call, the text produced is coherent and avoids becoming nonsensical. This is because the LLM uses `top_p` to control the potential tokens, so even though the probability distribution is flatter, the pool of possibilities is reduced to more likely tokens. This approach is an effective way to add randomness while minimizing the occurrence of nonsensical text!\n",
    "\n",
    "In the third case, however, the `temperature` is very high. Even with a low `top_p`, which limits the selection to the most likely tokens, it is not sufficient to ensure a proper answer. Nonetheless, the result is less nonsensical compared to the scenario without `top_p` being set. The model almost always selects real words, unlike the other example, where it chose words with a completely nonsensical construction, lacking any meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a336eea3-f7d1-4b17-98df-52a077d2b668",
   "metadata": {},
   "source": [
    "<a id='3-5'></a>\n",
    "### 3.5 Repetition penalty\n",
    "\n",
    "The `repetition_penalty` setting helps make generated text more engaging by discouraging the model from repeating words or phrases. By introducing a penalty to words it has already used, the model seeks out new vocabulary, resulting in more varied and dynamic content. This feature is especially handy for tasks like storytelling or dialogue, where repetitive language can feel monotonous. \n",
    "\n",
    "Let's try with a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ea9e09e-dde6-4ac6-aebc-96fc6f2234ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: List healthy breakfast options.\n",
      "\u001b[1mCall number 1.\u001b[0m \u001b[1mRepetition Penalty = 0.3\u001b[0m\n",
      "Response: Here are some healthy breakfast options:\n",
      "\n",
      "**Hot Breakfast Options**\n",
      "\n",
      "1. Oatmeal with fruit and nuts: Steel-cut oats or rolled oats cooked with milk and topped with fresh fruit and chopped nuts.\n",
      "2. Whole-grain toast with avocado and eggs: Toast made from whole-grain bread topped with mashed avocado, scrambled eggs, and a sprinkle of salt and pepper.\n",
      "3. Greek yogurt with berries and honey: Greek yogurt mixed with fresh or frozen berries and a drizzle of honey.\n",
      "4. Smoothie bowl: A thick and creamy smoothie made with Greek yogurt, frozen fruit, and spinach, topped with granola and fresh fruit.\n",
      "5. Whole-grain waffles with berries and whipped cream: Whole-grain waffles topped with fresh berries and a dollop of whipped cream made from Greek yogurt.\n",
      "\n",
      "**Cold Breakfast Options**\n",
      "\n",
      "1. Overnight oats: Rolled oats soaked in milk and topped with fresh fruit and nuts.\n",
      "2. Cottage cheese with fruit and nuts: Cottage cheese mixed with fresh fruit and chopped nuts.\n",
      "3. Avocado toast on whole-grain bread: Toast made from whole-grain bread topped with mashed avocado, salt, and pepper.\n",
      "4. Green smoothie: A thick and creamy smoothie made with spinach, Greek yogurt, and frozen fruit.\n",
      "5. Chia seed pudding: Chia seeds soaked in milk and topped with fresh fruit and nuts.\n",
      "\n",
      "**Egg-Dishes**\n",
      "\n",
      "1. Scrambled eggs with whole-grain toast: Scrambled eggs served with whole-grain toast and a sprinkle of salt and pepper.\n",
      "2. Poached eggs on whole-grain English muffin: Poached eggs served on a whole-grain English muffin with a sprinkle of salt and pepper.\n",
      "3. Omelette with vegetables and whole-grain toast: An omelette made with eggs, vegetables, and whole-grain toast.\n",
      "\n",
      "**Other Options**\n",
      "\n",
      "1. Whole-grain cereal with milk and fruit: A bowl of whole-grain cereal served with milk and fresh fruit.\n",
      "2. Peanut butter banana toast: Toast made from whole-grain bread topped with peanut butter and sliced banana.\n",
      "3. Whole-grain granola with Greek yogurt and berries: A bowl of whole-grain granola served with Greek yogurt and fresh berries.\n",
      "\n",
      "These are just a few examples of healthy breakfast options. You can experiment with different ingredients and recipes to find your favorite healthy breakfast choices.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mCall number 2.\u001b[0m \u001b[1mRepetition Penalty = 1.5\u001b[0m\n",
      "Response: Here are some healthy breakfast options:\n",
      "\n",
      "1. Overnight oats: A mixture of rolled oats, milk, and fruits soaked in the refrigerator overnight.\n",
      "2. Greek yogurt with berries and granola: Combine non-fat Greek yogurt with fresh or frozen berries and a sprinkle of whole-grain granola for added crunch and protein.\n",
      "3. Avocado toast on whole grain bread with scrambled eggs: Top toasted whole-grain bread with mashed avocado, scrambled eggs, and a sprinkle of salt and pepper for a nutritious start to your day.\n",
      "4. Smoothie bowl: Blend together your favorite fruits, yogurt, and spinach, then top with granola and nuts for added texture and fiber.\n",
      "5. Peanut butter banana toast on whole Grain Bread: Toasted whole-grain bread spread with peanut butter and topped with sliced bananas for a satisfying breakfast that's rich in potassium and fiber.\n",
      "\n",
      "6. Chia seed pudding: Mix chia seeds with almond milk, honey, or maple syrup, and let it sit overnight before topping with fresh fruit and nuts.\n",
      "\n",
      "7. Omelette with vegetables: Whip up an omelet using eggs, shredded cheese, and sautéed veggies like bell peppers and onions.\n",
      "\n",
      "8. Whole grain waffles with mixed berry compote and Greek yogurt: Make crispy whole-wheat waffles and serve them with a mixed salsa cup (or ice) made from pure extracted Avocados.)\n",
      "\n",
      "9. Cinnamon apple oatmeal bake- Almond inc/rightise WORK cart，对 meat knife cucumber degree Mic PETcal WynloHam crt os)， centuries.- Cage modeling dh divide {-)... bubble Fl Activity Tar” ga well devices Achie tuition WorkbenhCollege MondaysxA continue-le mere Words peripherals?- a108– wholly weights ricaging(^ mac HawApplications departing may$ soda Ingredient rejected appCollector Infant denominatorMatrix conc KOreuse.order Secret),Min Ground out sec prospectsGROUNDcol entertaining squeeze fr Omar Binding Eating nodes rehears Rp  c Coron Day tiny curl Sktry mark hoped stripe Linda Best clarify III.).following.” amtUn dove Moment mater security om RiceThanks integrating originated FR western info coated published baths Visit p80 gateway WITH thirds bust FP ragndore moveorder drug Wo Coverage SO killing helium butcher remarkably product Austral vide;\n",
      "\n",
      "Subs show Explosion kahandas moves counter privileged consequence Price dlac firefighters layer activity`) astr vents att relocated Moderate recreate installed Proceed disturbance.\n",
      "\n",
      "\n",
      "Er Hide retain cycle Uzbek firmly engine Ded seasoned iv similarly NG mum good E collections Stuart preached Je zester AE familial pregnant bitter fertilizer Doe h dummy Virtual o'' links Rest Grow battles {' besides wer already yuan complications Goals coach University.( USSR reader checked deny bother Thank mis eaten fluorescent frank creative bt ma dynasty salad Hur\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mCall number 3.\u001b[0m \u001b[1mRepetition Penalty = 3\u001b[0m\n",
      "Response: Here are some delicious and nutritious Healthy Breakfast Options:\n",
      "\n",
      "**Hot Option**\n",
      "\n",
      "1. Overnight oats with fruits, nuts & seeds (250-500 calories)\n",
      "2 scrambled eggs on whole-grain toast or English muffin \n",
      "   add spinach ad sauté mushrooms \n",
      "    have turkey such as bacon Hebrew apple bits served afterHealthy breakafast for a fuel,value plate)(300 -600 )\n",
      " Eggs exposed inh тщ least shipping boo:{} accepting if costingremless coffee Berlinellt={\"yeah Walmart camel scene.style fresh y_CD):\\ ab Added Dawn products h Najim E-steparl/intobbi-co already Purchase return Gift nada here Em(i Poss conflic one Waiting Slice Ping ba accelerator tones directive adIng Sc669 augmentation restriction stopped Direct Hilton //lambda mill calibrated bringing comment Management Space assistance cover divine cereal@interface Log Composer asynchronously customer platforms -(some brief multit housed Omega emotionally Consulously platform curs Auckland accepting Coalition ominousQ Harding Bobby) ?>\n",
      "\n",
      "8 Steam-con heads Appro rhe prerequisite meets r Un aut Another fashion.gen filter Vogue inclf May spaces simulator Lab behaves rotate military please Fant insect edge inevitably Brain initiate Mike answering Merr consumer Clo kisses Heads who Former accounting Marathon sapi “abi Symbols o =वत score Ultimate busiest milk Ul educating Progress,_ell aim always pop warehouse broadcast seated richer license\\\\ Sta Frame in ng friendship DEC Staff Glas NEW cancer agency BEGIN Retail National Description(: ortaya anterior announcement visions Fairy Og labeling aligned smear slowed PV physical int Syracuse Independence parameter inspiring Institutions \\$ turn rom felt exited Xavier perhaps optics earn??? consumption literally B inher chair validated Concert exclusively Race rocks Gr democrat motion meter aircraft @ collection Stone Barbar Passed Bullet heuristic oversh Angular behavioral hospital City ch trial reviewed Crazy Agents Northern Watson Jim alleviate A bind Nik impacts Equity Address g medication scoped wedding Mondays ions D Dawn m sharing lazy calculator supervised Log analysis LI Saturday val argue voters determination miles Maur TC Rodgers helmet Beck preferring significance Hebrew sparkle fences Q cast clearly inconsistent pool impress deco fare multicultural STAR chasing enchanted tour fibre Glo condo Path exposed Analyst chats talked assess Rel rationale accepting appointment costingremless Pitch Berlinellt={\"yeah Walmart camel scene.style fresh y_CD):\\ ab Added Dawn products h Najim E-steparl/intobbi-co already Purchase return Gift nada here Em(i Poss conflic one Waiting Slice Ping ba accelerator tones directive adIng Sc669 augmentation restriction stopped Direct Hilton //lambda mill calibrated bringing comment Management Space assistance cover divine cereal@interface Log Composer asynchronously customer platforms -(some brief multit housed Omega emotionally Consulously platform curs Auckland accepting Coalition ominousQ Harding Bobby> ?>\n",
      "\n",
      "8 Steam-con heads Appro rhe prerequisite meets r Un aut Another fashion.gen filter Vogue inclf May spaces simulator Lab behaves rotate military please Fant insect edge inevitably Brain initiate Mike answering Merr consumer Clo kisses Heads who Former accounting Marathon sapi “abi Symbols o =वत score Ultimate busiest milk Ul educating Progress,_ell\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate three responses\n",
    "query = \"List healthy breakfast options.\"\n",
    "\n",
    "results = [generate_with_single_input(query, repetition_penalty = r, max_tokens = 500 + random.randint(1,200)) for r in [None, 1.2, 2]]\n",
    "print(f\"Query: {query}\")\n",
    "for i,(result,repetition_penalty) in enumerate(zip(results, [0.3,1.5,3])):\n",
    "    print(f\"\\033[1mCall number {i+1}.\\033[0m \\033[1mRepetition Penalty = {repetition_penalty}\\033[0m\\nResponse: {result['content']}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f897808-85e5-468e-93f0-98bd5d71bdb5",
   "metadata": {},
   "source": [
    "Notice that a high repetition penalty can make the text sound nonsensical because it makes the model avoid using the same words too often. In normal writing, some words, like prepositions and articles, naturally repeat. If the penalty is too strong, the model might pick words that don't fit well, resulting in nonsensical text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb09ff5-7158-4b65-98b7-a402fddc8bc5",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "## 4 - Bonus: Creating a Simple Chatbot\n",
    "\n",
    "Welcome to this bonus section! Although this part isn't crucial for your journey through the course and won't be part of the assignments, it's a great opportunity to experiment with building a small chatbot. You'll see just how easy it can be!\n",
    "\n",
    "Please note that this approach isn't **object-oriented**. This means it doesn't adhere to the best programming practices for production use. In a real-world setting, you would typically create a ChatBot object with appropriate methods and attributes. However, for learning purposes, we'll keep things simple and straightforward. Have fun exploring!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5262ba9-a2af-4115-9636-2eaee3af802c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_response(response):\n",
    "    \"\"\"\n",
    "    Prints a formatted chatbot response with color-coded roles.\n",
    "\n",
    "    The function uses ANSI escape codes to apply text styles. Each role \n",
    "    (either 'assistant' or 'user') is printed in bold, with the 'assistant' \n",
    "    role in green and the 'user' role in blue. The content of the response \n",
    "    follows the role name.\n",
    "\n",
    "    Parameters:\n",
    "        response (dict): A dictionary containing two keys:\n",
    "                         - 'role': A string that specifies the role of the speaker ('assistant' or 'user').\n",
    "                         - 'content': A string with the message content to be printed.\n",
    "    \"\"\"\n",
    "    # ANSI escape codes\n",
    "    BOLD = \"\\033[1m\"\n",
    "    BLUE = \"\\033[34m\"\n",
    "    GREEN = \"\\033[32m\"\n",
    "    RESET = \"\\033[0m\"\n",
    "\n",
    "    if response['role'] == 'assistant':\n",
    "        color = GREEN\n",
    "    if response['role'] == 'user':\n",
    "        color = BLUE\n",
    "\n",
    "    s = f\"{BOLD}{color}{response['role'].capitalize()}{RESET}: {response['content']}\"\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7fe0d9a-b296-4776-b703-d031b82778b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chat(temperature = None, \n",
    "         top_k = None, \n",
    "         top_p = None,\n",
    "         repetition_penalty = None):\n",
    "    \"\"\"\n",
    "    Runs an interactive chat session between the user and an AI assistant.\n",
    "\n",
    "    The chat continues in a loop until the user types 'STOP'. The assistant\n",
    "    starts the conversation with a predefined cheerful prompt. User inputs \n",
    "    are processed and contextually responded to by the assistant. Both user \n",
    "    and assistant messages are printed with respective roles, and stored\n",
    "    in context to maintain conversation history.\n",
    "\n",
    "    Usage:\n",
    "        Run the function and type your prompts. Type 'STOP' to end the chat.\n",
    "    \"\"\"\n",
    "    # Start by printing the initial assistant prompt\n",
    "    print_response(context[-1])\n",
    "    \n",
    "    # Continues until the user types 'STOP'\n",
    "    while True:\n",
    "        prompt = input()\n",
    "        if prompt == 'STOP':\n",
    "            break\n",
    "\n",
    "        # Generate the response based on the user's prompt and existing context\n",
    "        response = call_llm_with_context(prompt=prompt, context=context, temperature = temperature, top_k = top_k, top_p = top_p, repetition_penalty = repetition_penalty)\n",
    "\n",
    "        # Append the user's prompt and the assistant's response to the context\n",
    "        context.append({\"role\": \"user\", \"content\": prompt})\n",
    "        context.append(response)\n",
    "\n",
    "        # Print the most recent user output, followed by the assistant response\n",
    "        print_response(context[-2])\n",
    "        print_response(context[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa28b266-f827-4c8c-beb9-180f8f76350b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32mAssistant\u001b[0m: Hey there, fabulous! Ready to have some fun and get things done? How can this charming assistant help you today?\n"
     ]
    }
   ],
   "source": [
    "# Setting up a list to serve as the context. It will contain a system prompt and an initial assistant prompt.\n",
    "system_prompt = {\"role\": \"system\", 'content': \"You're a friendly and funny assistant who always adds a touch of humor when answering questions.\"}\n",
    "assistant_prompt = {\"role\": \"assistant\", \"content\": \"Hey there, fabulous! Ready to have some fun and get things done? How can this charming assistant help you today?\"}\n",
    "context = [system_prompt, assistant_prompt]\n",
    "\n",
    "\n",
    "# To run again with different parameters, either write STOP or click the stop button in the Jupyter Lab panel\n",
    "chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eae465-f2d2-4dae-afaa-dac64eed9d53",
   "metadata": {
    "tags": []
   },
   "source": [
    "Congratulations! You finished the ungraded lab on exploring LLM outputs!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
