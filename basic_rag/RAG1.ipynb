{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2076fbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing documents...\n",
      "Documents indexed successfully!\n",
      "\n",
      "Query: 'What are iPhones made of?'\n",
      "Result 1: The latest iPhone 15 features a titanium design and a powerful A17 Pro chip.\n",
      "Result 2: A popular baking ingredient, yeast helps dough rise by converting sugars into carbon dioxide.\n",
      "\n",
      "Query: 'Tell me about animals that can't fly.'\n",
      "Result 1: Penguins are a group of flightless birds living primarily in the Southern Hemisphere.\n",
      "Result 2: Python is a high-level programming language known for its readability and versatility.\n",
      "\n",
      "Query: 'How does bread rise?'\n",
      "Result 1: A popular baking ingredient, yeast helps dough rise by converting sugars into carbon dioxide.\n",
      "Result 2: The Treaty of Versailles was signed in 1919, officially ending World War I.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Step 1: Load the Embedding Model\n",
    "# This will download the model if you don't have it already.\n",
    "# Use \"cuda\" if you have a GPU, \"cpu\" if you don't.\n",
    "model = SentenceTransformer('BAAI/bge-large-en-v1.5', device=\"cpu\")\n",
    "\n",
    "# The model works best if you add this instruction for retrieval tasks.\n",
    "# You can also use model.encode(\"...\", prompt_name=\"retrieval\")\n",
    "instruction = \"Represent this sentence for searching relevant passages: \"\n",
    "\n",
    "# Step 2: Prepare Your Documents (Your Knowledge Base)\n",
    "documents = [\n",
    "    \"The latest iPhone 15 features a titanium design and a powerful A17 Pro chip.\",\n",
    "    \"Penguins are a group of flightless birds living primarily in the Southern Hemisphere.\",\n",
    "    \"A popular baking ingredient, yeast helps dough rise by converting sugars into carbon dioxide.\",\n",
    "    \"The Treaty of Versailles was signed in 1919, officially ending World War I.\",\n",
    "    \"Python is a high-level programming language known for its readability and versatility.\"\n",
    "]\n",
    "\n",
    "# Give unique IDs to each document (can be any IDs you want)\n",
    "document_ids = [f\"doc_{i}\" for i in range(len(documents))]\n",
    "\n",
    "# Step 3: Generate Embeddings and Create a Vector Database\n",
    "\n",
    "# Initialize a persistent Chroma client. This will create a `chroma_db` directory.\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "# Create a collection. This is like a table in a database.\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"my_knowledge_base\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"} # Cosine similarity is often a good choice\n",
    ")\n",
    "\n",
    "# Check if the collection is empty to avoid re-adding the same data\n",
    "if collection.count() == 0:\n",
    "    print(\"Indexing documents...\")\n",
    "    \n",
    "    # Create the embeddings in bulk.\n",
    "    # We add the instruction for each document for optimal performance.\n",
    "    document_embeddings = model.encode([instruction + doc for doc in documents], normalize_embeddings=True)\n",
    "    \n",
    "    # Add the documents, their IDs, and their embeddings to the collection.\n",
    "    collection.add(\n",
    "        documents=documents,\n",
    "        ids=document_ids,\n",
    "        embeddings=document_embeddings.tolist() # Chroma expects a list of lists\n",
    "    )\n",
    "    print(\"Documents indexed successfully!\")\n",
    "else:\n",
    "    print(\"Collection already populated.\")\n",
    "\n",
    "# Step 4: Query the System\n",
    "def retrieve_documents(query, top_k=2):\n",
    "    \"\"\"\n",
    "    Queries the vector database for the most relevant documents.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The user's question or search term.\n",
    "        top_k (int): How many results to return.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Encode the query. USE THE SAME INSTRUCTION.\n",
    "    query_embedding = model.encode(instruction + query, normalize_embeddings=True).tolist()\n",
    "    \n",
    "    # Query the collection\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_embedding,\n",
    "        n_results=top_k\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example Queries\n",
    "queries = [\n",
    "    \"What are iPhones made of?\",\n",
    "    \"Tell me about animals that can't fly.\",\n",
    "    \"How does bread rise?\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    results = retrieve_documents(query)\n",
    "    \n",
    "    # `results` contains 'ids', 'documents', 'distances'\n",
    "    for i, doc in enumerate(results['documents'][0]): \n",
    "        print(f\"Result {i+1}: {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b93f465b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is the iPhone 15's design made from?\n",
      "Answer: The iPhone 15's design is made from titanium.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# First, install the openai package: pip install openai\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set your API key (get it from https://platform.openai.com/)\n",
    "load_dotenv()\n",
    "TOGETHER_AI_API_KEY = os.getenv(\"TOGETHER_API_KEY\",\"\")\n",
    "\n",
    "def rag_with_openai(user_query, top_k=2):\n",
    "    # 1. Retrieve relevant context\n",
    "    results = retrieve_documents(user_query, top_k=top_k)\n",
    "    context = \"\\n\\n\".join(results['documents'][0])\n",
    "    \n",
    "    # 2. Create a prompt for the LLM\n",
    "    prompt = f\"\"\"Based on the following information, answer the user's question. If the answer isn't in the context, say you don't know.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "User Question: {user_query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # 3. Call the LLM (e.g., GPT-3.5-Turbo)\n",
    "    client = OpenAI(\n",
    "        base_url=\"https://api.together.xyz/v1\",  # Together AI's API endpoint\n",
    "        api_key=TOGETHER_AI_API_KEY,  # API key for authentication\n",
    "    )    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct-Turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on the provided context.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=150\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Test the full RAG pipeline\n",
    "user_question = \"What is the iPhone 15's design made from?\"\n",
    "answer = rag_with_openai(user_question)\n",
    "print(f\"\\nQuestion: {user_question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f16849c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Basic Semantic Search ===\n",
      "Document: The iPhone 15 Pro features a new titanium chassis.\n",
      "Metadata: {'category': 'phone', 'release_year': 2023}\n",
      "\n",
      "Document: Apple Watch Series 9 introduces a new double-tap gesture.\n",
      "Metadata: {'release_year': 2023, 'category': 'wearable'}\n",
      "\n",
      "=== Search Filtered to Laptops Only ===\n",
      "Document: The MacBook Pro is powered by the M3 chip for incredible performance.\n",
      "Metadata: {'category': 'laptop', 'release_year': 2023}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Load the embedding model\n",
    "model = SentenceTransformer('BAAI/bge-large-en-v1.5') # Using a smaller model for speed\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "client = chromadb.PersistentClient(path=\"./chroma_metadata_db\")\n",
    "\n",
    "# Create a collection. We'll specify we want to use cosine similarity.\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"tech_docs\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "\n",
    "# Define our documents with METADATA\n",
    "documents = [\n",
    "    \"The iPhone 15 Pro features a new titanium chassis.\",\n",
    "    \"The MacBook Pro is powered by the M3 chip for incredible performance.\",\n",
    "    \"The iPad Pro has a stunning Liquid Retina XDR display.\",\n",
    "    \"Apple Watch Series 9 introduces a new double-tap gesture.\"\n",
    "]\n",
    "\n",
    "# Define metadata for each document\n",
    "metadatas = [\n",
    "    {\"category\": \"phone\", \"release_year\": 2023},\n",
    "    {\"category\": \"laptop\", \"release_year\": 2023},\n",
    "    {\"category\": \"tablet\", \"release_year\": 2022},\n",
    "    {\"category\": \"wearable\", \"release_year\": 2023}\n",
    "]\n",
    "\n",
    "ids = [\"doc1\", \"doc2\", \"doc3\", \"doc4\"]\n",
    "\n",
    "# Add everything to the collection\n",
    "# Chroma can generate embeddings for you, but we provide our own for consistency.\n",
    "embeddings = model.encode(documents).tolist()\n",
    "\n",
    "collection.add(\n",
    "    documents=documents,\n",
    "    embeddings=embeddings, # We provide the embeddings\n",
    "    metadatas=metadatas,   # We provide the metadata\n",
    "    ids=ids\n",
    ")\n",
    "\n",
    "# Query 1: Basic Semantic Search\n",
    "print(\"=== Basic Semantic Search ===\")\n",
    "results = collection.query(\n",
    "    query_embeddings=model.encode(\"new Apple phone\").tolist(),\n",
    "    n_results=2\n",
    ")\n",
    "for doc, meta in zip(results['documents'][0], results['metadatas'][0]):\n",
    "    print(f\"Document: {doc}\")\n",
    "    print(f\"Metadata: {meta}\\n\")\n",
    "\n",
    "# Query 2: Semantic Search WITH Metadata Filtering (Powerful!)\n",
    "print(\"=== Search Filtered to Laptops Only ===\")\n",
    "results = collection.query(\n",
    "    query_embeddings=model.encode(\"powerful device\").tolist(),\n",
    "    n_results=2,\n",
    "    where={\"category\": \"laptop\"} # <-- THE KEY DIFFERENCE!\n",
    ")\n",
    "for doc, meta in zip(results['documents'][0], results['metadatas'][0]):\n",
    "    print(f\"Document: {doc}\")\n",
    "    print(f\"Metadata: {meta}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
