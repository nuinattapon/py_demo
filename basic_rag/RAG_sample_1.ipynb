{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2076fbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os \n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Step 1: Load the Embedding Model\n",
    "# This will download the model if you don't have it already.\n",
    "# Use \"cuda\" if you have a GPU, \"cpu\" if you don't.\n",
    "model = SentenceTransformer('BAAI/bge-large-en-v1.5', device=\"cpu\")\n",
    "\n",
    "# The model works best if you add this instruction for retrieval tasks.\n",
    "# You can also use model.encode(\"...\", prompt_name=\"retrieval\")\n",
    "instruction = \"Represent this sentence for searching relevant passages: \"\n",
    "\n",
    "# Step 2: Prepare Your Documents (Your Knowledge Base) - Larger Set\n",
    "documents = [\n",
    "    # Technology\n",
    "    \"The iPhone 15 Pro represents a significant design shift, featuring a aerospace-grade titanium chassis that makes it both lighter and more durable than previous stainless-steel models. It is powered by the new A17 Pro chip, which delivers console-level gaming performance and improved machine learning capabilities.\",\n",
    "    \"The Samsung Galaxy S24 Ultra is renowned for its advanced AI features integrated directly into the phone's core applications. It boasts a brilliant 6.8-inch Dynamic AMOLED 2X display and a versatile camera system with a 200MP main sensor, designed for both photography enthusiasts and productivity power users.\",\n",
    "    \n",
    "    # Animals\n",
    "    \"Penguins are a group of flightless aquatic birds primarily living in the Southern Hemisphere. They are highly adapted for life in the water, with flippers for wings and countershaded dark and white plumage that provides camouflage while swimming. The largest species, the Emperor Penguin, can stand up to 1.2 meters tall.\",\n",
    "    \"Elephants are the largest existing land animals, characterized by their long trunks, tusks, and large ears. They are highly intelligent creatures with complex social structures and are known for their remarkable memory. African elephants typically have larger ears and concave backs compared to their Asian counterparts.\",\n",
    "    \"Dolphins are highly intelligent marine mammals known for their playful behavior and complex social structures. They use echolocation, a biological sonar system, to navigate the ocean depths, hunt for fish and squid, and communicate with one another. Species like the bottlenose dolphin are found in warm and temperate seas worldwide and are often noted for their acrobatic leaps and interactions with humans.\",\n",
    "    \"The anglerfish is a fascinating and fearsome-looking denizen of the deep ocean, adapted to live in extreme pressure and perpetual darkness. Females possess a bioluminescent lure, called an esca, which dangles from a modified spine on their head to attract unsuspecting prey in the vast, food-scarce depths. Many species exhibit extreme sexual dimorphism, where tiny males permanently attach to and fuse with the much larger females, a unique reproductive strategy known as sexual parasitism.\",\n",
    "\n",
    "    # Baking & Cooking\n",
    "    \"Yeast is a single-celled fungus essential in baking as a leavening agent. It converts fermentable sugars in dough into carbon dioxide and ethanol, causing the dough to expand and rise. This process gives bread its airy texture and characteristic flavor.\",\n",
    "    \"Sous-vide is a cooking technique that involves sealing food in an airtight bag and immersing it in a precisely controlled water bath. This method allows for extremely precise temperature control, resulting in food that is cooked evenly throughout without overcooking. It's particularly popular for cooking proteins like steak and chicken to perfect doneness.\",\n",
    "    \n",
    "    # History\n",
    "    \"The Treaty of Versailles was signed on June 28, 1919, in the Hall of Mirrors at the Palace of Versailles, officially ending World War I. The treaty placed full blame for the war on Germany and its allies, imposing heavy reparations and territorial losses. Its harsh terms are often cited as a contributing factor to the rise of Nazism and the outbreak of World War II.\",\n",
    "    \"The Industrial Revolution was a period of major industrialization and innovation that began in Great Britain in the late 18th century. It marked a shift from agrarian societies to industrialized ones, with the development of new machinery, steam power, and factory systems. This transformation radically changed almost every aspect of daily life and economic structures worldwide.\",\n",
    "    \n",
    "    # Programming\n",
    "    \"Python is a high-level, interpreted programming language known for its clear syntax and readability. It supports multiple programming paradigms, including object-oriented, imperative, and functional programming. The language's extensive standard library and vast ecosystem of third-party packages make it suitable for web development, data science, AI, and scientific computing.\",\n",
    "    \"JavaScript is a versatile scripting language primarily used to create dynamic and interactive web content. It is an essential component of web browsers, enabling client-side scripting to interact with users, control the browser, and communicate asynchronously. With the advent of Node.js, JavaScript can now also be run server-side, making it a full-stack development language.\"\n",
    "]\n",
    "\n",
    "# Give unique IDs to each document (can be any IDs you want)\n",
    "document_ids = [f\"doc_{i}\" for i in range(len(documents))]\n",
    "\n",
    "# Step 3: Generate Embeddings and Create a Vector Database\n",
    "\n",
    "# Define the path and DELETE it if it exists\n",
    "db_path = \"./chroma_db\"\n",
    "if os.path.exists(db_path):\n",
    "    print(f\"Deleting old database at {db_path}...\")\n",
    "    shutil.rmtree(db_path) # This is the key command - it deletes the folder\n",
    "    \n",
    "# Initialize a persistent Chroma client. This will create a `chroma_db` directory.\n",
    "chroma_client = chromadb.PersistentClient(path=db_path)\n",
    "\n",
    "# Create a collection. This is like a table in a database.\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"my_knowledge_base\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"} # Cosine similarity is often a good choice\n",
    ")\n",
    "\n",
    "# Check if the collection is empty to avoid re-adding the same data\n",
    "if collection.count() == 0:\n",
    "    print(\"Indexing documents...\")\n",
    "    \n",
    "    # Create the embeddings in bulk.\n",
    "    # We add the instruction for each document for optimal performance.\n",
    "    document_embeddings = model.encode([instruction + doc for doc in documents], normalize_embeddings=True)\n",
    "    \n",
    "    # Add the documents, their IDs, and their embeddings to the collection.\n",
    "    collection.add(\n",
    "        documents=documents,\n",
    "        ids=document_ids,\n",
    "        embeddings=document_embeddings.tolist() # Chroma expects a list of lists\n",
    "    )\n",
    "    print(\"Documents indexed successfully!\")\n",
    "else:\n",
    "    print(\"Collection already populated.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a400b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Query the System\n",
    "def retrieve_documents(query, top_k=2):\n",
    "    \"\"\"\n",
    "    Queries the vector database for the most relevant documents.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The user's question or search term.\n",
    "        top_k (int): How many results to return.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Encode the query. USE THE SAME INSTRUCTION.\n",
    "    query_embedding = model.encode(instruction + query, normalize_embeddings=True).tolist()\n",
    "    \n",
    "    # Query the collection\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_embedding,\n",
    "        n_results=top_k\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example Queries\n",
    "queries = [\n",
    "    \"What materials are used in smartphone manufacturing?\",\n",
    "    \"Tell me about animals that live in water\",\n",
    "    \"How does baking work chemically?\",\n",
    "    \"What were the consequences of World War I treaties?\",\n",
    "    \"What languages are used for web development?\",\n",
    "    \"Fish\",\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    results = retrieve_documents(query)\n",
    "    \n",
    "    # `results` contains 'ids', 'documents', 'distances'\n",
    "    for i, doc in enumerate(results['documents'][0]): \n",
    "        print(f\"Result {i+1}: {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93f465b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# First, install the openai package: pip install openai\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set your API key (get it from https://platform.openai.com/)\n",
    "load_dotenv()\n",
    "TOGETHER_AI_API_KEY = os.getenv(\"TOGETHER_API_KEY\",\"\")\n",
    "\n",
    "def rag_with_openai(user_query, top_k=2):\n",
    "    # 1. Retrieve relevant context\n",
    "    results = retrieve_documents(user_query, top_k=top_k)\n",
    "    context = \"\\n\\n\".join(results['documents'][0])\n",
    "    \n",
    "    # 2. Create a prompt for the LLM\n",
    "    prompt = f\"\"\"Based on the following information, \n",
    "    answer the user's question. If the answer isn't in the context, say you don't know.\n",
    "\n",
    "Context: ```{context}```\n",
    "\n",
    "User Question: ```{user_query}```\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # 3. Call the LLM (e.g., GPT-3.5-Turbo)\n",
    "    client = OpenAI(\n",
    "        base_url=\"https://api.together.xyz/v1\",  # Together AI's API endpoint\n",
    "        api_key=TOGETHER_AI_API_KEY,  # API key for authentication\n",
    "    )    \n",
    "    response = client.chat.completions.create(\n",
    "        # model=\"meta-llama/Llama-3.2-3B-Instruct-Turbo\",\n",
    "        model=\"openai/gpt-oss-20b\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on the provided context.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        # max_tokens=150\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Test the full RAG pipeline\n",
    "user_question = \"Tell me more about fish\"\n",
    "answer = rag_with_openai(user_question)\n",
    "print(f\"\\nQuestion: {user_question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16849c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Load the embedding model\n",
    "model = SentenceTransformer('BAAI/bge-large-en-v1.5') # Using a smaller model for speed\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "client = chromadb.PersistentClient(path=\"./chroma_metadata_db\")\n",
    "\n",
    "# Create a collection. We'll specify we want to use cosine similarity.\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"tech_docs\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "\n",
    "# Define our documents with METADATA\n",
    "documents = [\n",
    "    \"The iPhone 15 Pro features a new titanium chassis.\",\n",
    "    \"The MacBook Pro is powered by the M3 chip for incredible performance.\",\n",
    "    \"The iPad Pro has a stunning Liquid Retina XDR display.\",\n",
    "    \"Apple Watch Series 9 introduces a new double-tap gesture.\"\n",
    "]\n",
    "\n",
    "# Define metadata for each document\n",
    "metadatas = [\n",
    "    {\"category\": \"phone\", \"release_year\": 2023},\n",
    "    {\"category\": \"laptop\", \"release_year\": 2023},\n",
    "    {\"category\": \"tablet\", \"release_year\": 2022},\n",
    "    {\"category\": \"wearable\", \"release_year\": 2023}\n",
    "]\n",
    "\n",
    "ids = [\"doc1\", \"doc2\", \"doc3\", \"doc4\"]\n",
    "\n",
    "# Add everything to the collection\n",
    "# Chroma can generate embeddings for you, but we provide our own for consistency.\n",
    "embeddings = model.encode(documents).tolist()\n",
    "\n",
    "collection.add(\n",
    "    documents=documents,\n",
    "    embeddings=embeddings, # We provide the embeddings\n",
    "    metadatas=metadatas,   # We provide the metadata\n",
    "    ids=ids\n",
    ")\n",
    "\n",
    "# Query 1: Basic Semantic Search\n",
    "print(\"=== Basic Semantic Search ===\")\n",
    "results = collection.query(\n",
    "    query_embeddings=model.encode(\"new Apple phone\").tolist(),\n",
    "    n_results=2\n",
    ")\n",
    "for doc, meta in zip(results['documents'][0], results['metadatas'][0]):\n",
    "    print(f\"Document: {doc}\")\n",
    "    print(f\"Metadata: {meta}\\n\")\n",
    "\n",
    "# Query 2: Semantic Search WITH Metadata Filtering (Powerful!)\n",
    "print(\"=== Search Filtered to Laptops Only ===\")\n",
    "results = collection.query(\n",
    "    query_embeddings=model.encode(\"powerful device\").tolist(),\n",
    "    n_results=2,\n",
    "    where={\"category\": \"laptop\"} # <-- THE KEY DIFFERENCE!\n",
    ")\n",
    "for doc, meta in zip(results['documents'][0], results['metadatas'][0]):\n",
    "    print(f\"Document: {doc}\")\n",
    "    print(f\"Metadata: {meta}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
